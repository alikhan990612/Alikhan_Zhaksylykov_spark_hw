{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224f227e-2056-46f1-a61d-505441b409da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code imports the necessary modules to work with PySpark:\n",
    "\"\"\"\n",
    "SparkSession - to create a Spark session.\n",
    "reduce - to combine multiple DataFrames.\n",
    "DataFrame - for working with data in the form of tables.\n",
    "col - for accessing columns in PySpark expressions.\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1112ee17-7159-4835-9d22-a327a5848541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session with certain configuration parameters:\n",
    "\"\"\"\n",
    "appName(\"Spark_ETL\"): \n",
    "Sets the Spark application name to \"Spark_ETL\".\n",
    "\n",
    "config('spark.sql.shuffle.partitions', 200):\n",
    "Sets the number of partitions for shuffle operations (eg grouping, joining) to 200. Also a parameter for performance optimization.\n",
    "\"\"\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.io.compression.codec\", \"zstd\") \\\n",
    "    .config(\"spark.sql.execution.pythonUDF.arrow.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sol.shuffle.part-tions\", 200) \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"5\") \\\n",
    "    .config(\"spark.sql.files.maxRecordsPerFile\", 10_000) \\\n",
    "    .config(\"spark.debug.maxToStringFields\", 1000) \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"3600s\") \\\n",
    "    .config(\"spark.network.timeout\", \"7200s\") \\\n",
    "    .config(\"spark.network.timeoutInterval\", \"3600s\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85d09e8-43e1-4bb2-bc76-8ca3684f2be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['restaurant_datasets/restaurant_csv/part-00000-c8acc470-919e-4ea9-b274-11488238c85e-c000.csv',\n",
       " 'restaurant_datasets/restaurant_csv/part-00001-c8acc470-919e-4ea9-b274-11488238c85e-c000.csv',\n",
       " 'restaurant_datasets/restaurant_csv/part-00002-c8acc470-919e-4ea9-b274-11488238c85e-c000.csv',\n",
       " 'restaurant_datasets/restaurant_csv/part-00003-c8acc470-919e-4ea9-b274-11488238c85e-c000.csv',\n",
       " 'restaurant_datasets/restaurant_csv/part-00004-c8acc470-919e-4ea9-b274-11488238c85e-c000.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts: 00000 = 400, 00001 = 397, 00002 = 400, 00003 = 400, 00000 = 400,\n",
    "# Create a file_paths list containing the paths to five CSV files in the restaurant_datasets/restaurant_csv/ directory\n",
    "# Used this method instead of reading all excel files entirely. When I read all the files, it takes a long time to load. So I optimized it this way\n",
    "\n",
    "file_paths = [f'restaurant_datasets/restaurant_csv/part-0000{i}-c8acc470-919e-4ea9-b274-11488238c85e-c000.csv' for i in range(5)]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ede937-71bd-47c1-831f-8374263edb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataFrame[id: bigint, franchise_id: int, franchise_name: string, restaurant_franchise_id: int, country: string, city: string, lat: double, lng: double],\n",
       " DataFrame[id: bigint, franchise_id: int, franchise_name: string, restaurant_franchise_id: int, country: string, city: string, lat: double, lng: double],\n",
       " DataFrame[id: bigint, franchise_id: int, franchise_name: string, restaurant_franchise_id: int, country: string, city: string, lat: double, lng: double],\n",
       " DataFrame[id: bigint, franchise_id: int, franchise_name: string, restaurant_franchise_id: int, country: string, city: string, lat: double, lng: double],\n",
       " DataFrame[id: bigint, franchise_id: int, franchise_name: string, restaurant_franchise_id: int, country: string, city: string, lat: double, lng: double]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all files and combine them into one DataFrame\n",
    "dfs = [spark.read.csv(file_path, header=True, inferSchema=True) for file_path in file_paths]\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e0ab4d3-9fe3-4d05-9def-66b3f711f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+-----------------------+-------+------------------+------+--------+\n",
      "|          id|franchise_id|      franchise_name|restaurant_franchise_id|country|              city|   lat|     lng|\n",
      "+------------+------------+--------------------+-----------------------+-------+------------------+------+--------+\n",
      "|257698037796|          37|          Cafe Crepe|                  26468|     IT|             Milan|45.533|   9.171|\n",
      "| 25769803831|          56|    The Waffle House|                  72230|     FR|             Paris|48.873|   2.305|\n",
      "| 85899345988|          69|      Dragonfly Cafe|                  18952|     NL|         Amsterdam|52.392|   4.911|\n",
      "|111669149758|          63|          Cafe Paris|                  84488|     NL|Amsterdam Zuidoost| 52.31|   4.942|\n",
      "|163208757268|          21|      The Lazy Daisy|                  96638|     US|          Columbus|40.115| -83.015|\n",
      "|154618822662|           7|           Cafe Roma|                  41484|     US|             Tatum|33.382|-103.395|\n",
      "|163208757290|          43|      The Food House|                  96638|     IT|             Milan|45.474|   9.224|\n",
      "|266287972361|          10|    The Golden Spoon|                  11263|     US|            Marina|36.684|-121.792|\n",
      "|171798691894|          55|     The Steak House|                  65939|     GB|            London|51.502|     0.0|\n",
      "|197568495640|          25|       The Cozy Cafe|                  24784|     US|         Oskaloosa|41.324| -92.646|\n",
      "|163208757306|          59|         Azalea Cafe|                  96638|     FR|             Paris|48.871|   2.294|\n",
      "|          42|          43|      The Food House|                  47732|     AT|            Vienna|48.163|   16.34|\n",
      "|          23|          24|The Fisherman's C...|                  47732|     ES|         Barcelona|41.396|   2.163|\n",
      "| 51539607572|          21|      The Lazy Daisy|                   6934|     ES|         Barcelona|41.387|   2.174|\n",
      "|257698037775|          16|      The Spice Tree|                  26468|     US|        Morgantown|39.631| -79.956|\n",
      "|188978561036|          13|       The Firehouse|                   3642|     US|    Atlantic Beach|34.701| -76.747|\n",
      "| 77309411367|          40|        Crimson Cafe|                  78190|     AT|            Vienna|48.213|  16.357|\n",
      "|128849018902|          23|      The Hungry Pig|                   5679|     ES|         Barcelona|41.389|   2.171|\n",
      "|223338299392|           1|             Savoria|                  36937|     US|        Washington|13.368| 100.987|\n",
      "|103079215115|          12|    The Wooden Spoon|                   4340|     US|        Blythewood|34.214| -80.974|\n",
      "+------------+------------+--------------------+-----------------------+-------+------------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combining all DataFrames into one\n",
    "# Reduce is used to sequentially apply a union operation to all DataFrames in a list.\n",
    "combined_df = reduce(DataFrame.union, dfs)\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f0bd56-e6cf-4fd3-84f8-a7002a66526f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1997"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number\n",
    "record_count = combined_df.count()\n",
    "record_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ceb56cb-464f-4277-b653-ddbf109a3754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "|         id|franchise_id|franchise_name|restaurant_franchise_id|country|  city| lat| lng|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "|85899345920|           1|       Savoria|                  18952|     US|Dillon|NULL|NULL|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column \"lat\" for emptiness\n",
    "null_latitude_df = combined_df.filter(col(\"lat\").isNull())\n",
    "null_latitude_df.show()\n",
    "null_latitude_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4849283c-4df7-4a20-96b5-045e08118119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "|         id|franchise_id|franchise_name|restaurant_franchise_id|country|  city| lat| lng|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "|85899345920|           1|       Savoria|                  18952|     US|Dillon|NULL|NULL|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column \"lng\" for emptiness\n",
    "null_longitude_df = combined_df.filter(col(\"lng\").isNull())\n",
    "null_longitude_df.show()\n",
    "null_latitude_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16dcff63-a986-46f0-8caa-095419def011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.401409; -79.386434; us; America/New_York; Dillon County, South Carolina, United States of America\n"
     ]
    }
   ],
   "source": [
    "# Used ready-made code from the OpenCage Geocoding API website to check lat and lng values when transmitting addresses.\n",
    "\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "\n",
    "key = \"your_key\"\n",
    "geocoder = OpenCageGeocode(key)\n",
    "\n",
    "city_value = null_latitude_df.select(\"city\").first()[0]\n",
    "country_value = null_latitude_df.select(\"country\").first()[0]\n",
    "query = f'{city_value}, {country_value}'\n",
    "results = geocoder.geocode(query)\n",
    "\n",
    "print(u\"%f; %f; %s; %s; %s\" % (results[0][\"geometry\"][\"lat\"], \n",
    "                        results[0][\"geometry\"][\"lng\"],\n",
    "                        results[0][\"components\"][\"country_code\"],\n",
    "                        results[0][\"annotations\"][\"timezone\"][\"name\"],\n",
    "                        results[0][\"formatted\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57bfcce8-9390-41e9-95cd-8714d9f1f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Implements the geocode_address(address) function, \n",
    "which uses the OpenCage geocoding service API to convert an address to coordinates (latitude and longitude)\n",
    "\"\"\"\n",
    "import requests\n",
    "\n",
    "def geocode_address(address):\n",
    "    # my api_key and api url\n",
    "    api_key = \"your_key\"\n",
    "    api_url = f\"https://api.opencagedata.com/geocode/v1/json?q={address}&key={api_key}\"\n",
    "    # rest request\n",
    "    response = requests.get(api_url)\n",
    "    data = response.json()\n",
    "\n",
    "    if response.status_code == 200 and data.get(\"results\"):\n",
    "        # Returning coordinates from the first result\n",
    "        return data[\"results\"][0][\"geometry\"][\"lat\"], data[\"results\"][0][\"geometry\"][\"lng\"]\n",
    "    else:\n",
    "        # If we couldn't get the coordinates, return None\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b026185-701a-4aa3-8b64-d0fb97ae3303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dillon, US\n",
      "(34.4014089, -79.3864339)\n"
     ]
    }
   ],
   "source": [
    "# Select the city and country from the first line that have the empty value to check that the functions work correctly\n",
    "city_value = null_latitude_df.select(\"city\").first()[0]\n",
    "country_value = null_latitude_df.select(\"country\").first()[0]\n",
    "print(city_value + ', ' + country_value)\n",
    "\n",
    "query = f'{city_value}, {country_value}'\n",
    "print(geocode_address(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6319c393-003a-401f-b1c5-e0f07eb1ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a custom function that will be applied to the DataFrame columns to update the coordinate values. \n",
    "If latitude (lat) or longitude (lng) values are missing, the function uses geocoding to obtain new coordinates based on the city and country. \n",
    "If the values already exist, the function leaves them unchanged.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType, StructField, FloatType\n",
    "# Function to update values\n",
    "def update_coordinates(lat, lng, city_value, country_value):\n",
    "    if lat is None or lng is None:\n",
    "        # If the value is incorrect, update using the API\n",
    "        address = f'{city_value}, {country_value}'\n",
    "        new_lat, new_lng = geocode_address(address)\n",
    "        return new_lat, new_lng\n",
    "    else:\n",
    "        # If the value already exists, leave it unchanged\n",
    "        return lat, lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df0578e3-f779-46e2-9476-ada7669f2f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applies the update_coordinates function to the columns of the DataFrame. \n",
    "The StructType schema specifies that the UDF will return a structure with two fields: updated_latitude and updated_longitude. \n",
    "These fields will contain the updated latitude and longitude values respectively.\n",
    "\"\"\"\n",
    "update_coordinates_udf = udf(update_coordinates, StructType([\n",
    "    StructField(\"updated_latitude\", FloatType()),\n",
    "    StructField(\"updated_longitude\", FloatType())\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6814091-60d8-4cc8-a338-d16b0bf24d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column 'updated_coordinates' in DataFrame 'combined_df'\n",
    "updated_df = combined_df.withColumn(\"updated_coordinates\", update_coordinates_udf(\"lat\", \"lng\", \"city\", \"country\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ada531a-1597-43ba-8737-0be9af85ecf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+-----------------------+-------+------------------+------+--------+-------------------+\n",
      "|          id|franchise_id|      franchise_name|restaurant_franchise_id|country|              city|   lat|     lng|updated_coordinates|\n",
      "+------------+------------+--------------------+-----------------------+-------+------------------+------+--------+-------------------+\n",
      "|257698037796|          37|          Cafe Crepe|                  26468|     IT|             Milan|45.533|   9.171|    {45.533, 9.171}|\n",
      "| 25769803831|          56|    The Waffle House|                  72230|     FR|             Paris|48.873|   2.305|    {48.873, 2.305}|\n",
      "| 85899345988|          69|      Dragonfly Cafe|                  18952|     NL|         Amsterdam|52.392|   4.911|    {52.392, 4.911}|\n",
      "|111669149758|          63|          Cafe Paris|                  84488|     NL|Amsterdam Zuidoost| 52.31|   4.942|     {52.31, 4.942}|\n",
      "|163208757268|          21|      The Lazy Daisy|                  96638|     US|          Columbus|40.115| -83.015|  {40.115, -83.015}|\n",
      "|154618822662|           7|           Cafe Roma|                  41484|     US|             Tatum|33.382|-103.395| {33.382, -103.395}|\n",
      "|163208757290|          43|      The Food House|                  96638|     IT|             Milan|45.474|   9.224|    {45.474, 9.224}|\n",
      "|266287972361|          10|    The Golden Spoon|                  11263|     US|            Marina|36.684|-121.792| {36.684, -121.792}|\n",
      "|171798691894|          55|     The Steak House|                  65939|     GB|            London|51.502|     0.0|      {51.502, 0.0}|\n",
      "|197568495640|          25|       The Cozy Cafe|                  24784|     US|         Oskaloosa|41.324| -92.646|  {41.324, -92.646}|\n",
      "|163208757306|          59|         Azalea Cafe|                  96638|     FR|             Paris|48.871|   2.294|    {48.871, 2.294}|\n",
      "|          42|          43|      The Food House|                  47732|     AT|            Vienna|48.163|   16.34|    {48.163, 16.34}|\n",
      "|          23|          24|The Fisherman's C...|                  47732|     ES|         Barcelona|41.396|   2.163|    {41.396, 2.163}|\n",
      "| 51539607572|          21|      The Lazy Daisy|                   6934|     ES|         Barcelona|41.387|   2.174|    {41.387, 2.174}|\n",
      "|257698037775|          16|      The Spice Tree|                  26468|     US|        Morgantown|39.631| -79.956|  {39.631, -79.956}|\n",
      "|188978561036|          13|       The Firehouse|                   3642|     US|    Atlantic Beach|34.701| -76.747|  {34.701, -76.747}|\n",
      "| 77309411367|          40|        Crimson Cafe|                  78190|     AT|            Vienna|48.213|  16.357|   {48.213, 16.357}|\n",
      "|128849018902|          23|      The Hungry Pig|                   5679|     ES|         Barcelona|41.389|   2.171|    {41.389, 2.171}|\n",
      "|223338299392|           1|             Savoria|                  36937|     US|        Washington|13.368| 100.987|  {13.368, 100.987}|\n",
      "|103079215115|          12|    The Wooden Spoon|                   4340|     US|        Blythewood|34.214| -80.974|  {34.214, -80.974}|\n",
      "+------------+------------+--------------------+-----------------------+-------+------------------+------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c24fbd13-eef7-4bec-9d0f-3eb10dbd1a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "|         id|franchise_id|franchise_name|restaurant_franchise_id|country|  city| lat| lng|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "|85899345920|           1|       Savoria|                  18952|     US|Dillon|NULL|NULL|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Updates the values of the lat and lng columns in the DataFrame updated_df using information from the new updated_coordinates column\n",
    "updated_df = updated_df.withColumn(\"lat\", col(\"updated_coordinates.updated_latitude\"))\n",
    "updated_df = updated_df.withColumn(\"lng\", col(\"updated_coordinates.updated_longitude\"))\n",
    "\n",
    "# Deleting a column \"updated_coordinates\"\n",
    "updated_df = updated_df.drop(\"updated_coordinates\")\n",
    "\n",
    "updated_df.filter(col(\"id\") == 85899345920).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d653b0c3-5315-4f9a-a411-44de28d88353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "|         id|franchise_id|franchise_name|restaurant_franchise_id|country|  city| lat| lng|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "|85899345920|           1|       Savoria|                  18952|     US|Dillon|NULL|NULL|\n",
      "+-----------+------------+--------------+-----------------------+-------+------+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now check again for empty values\n",
    "null_longitude_df = updated_df.filter(col(\"lat\").isNull())\n",
    "null_longitude_df.show()\n",
    "null_latitude_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d62b7b8a-620d-491f-8478-f27ee6e70583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------------------+-----------------------+-------+------------------+------+--------+\n",
      "|id          |franchise_id|franchise_name       |restaurant_franchise_id|country|city              |lat   |lng     |\n",
      "+------------+------------+---------------------+-----------------------+-------+------------------+------+--------+\n",
      "|257698037796|37          |Cafe Crepe           |26468                  |IT     |Milan             |45.533|9.171   |\n",
      "|25769803831 |56          |The Waffle House     |72230                  |FR     |Paris             |48.873|2.305   |\n",
      "|85899345988 |69          |Dragonfly Cafe       |18952                  |NL     |Amsterdam         |52.392|4.911   |\n",
      "|111669149758|63          |Cafe Paris           |84488                  |NL     |Amsterdam Zuidoost|52.31 |4.942   |\n",
      "|163208757268|21          |The Lazy Daisy       |96638                  |US     |Columbus          |40.115|-83.015 |\n",
      "|154618822662|7           |Cafe Roma            |41484                  |US     |Tatum             |33.382|-103.395|\n",
      "|163208757290|43          |The Food House       |96638                  |IT     |Milan             |45.474|9.224   |\n",
      "|266287972361|10          |The Golden Spoon     |11263                  |US     |Marina            |36.684|-121.792|\n",
      "|171798691894|55          |The Steak House      |65939                  |GB     |London            |51.502|0.0     |\n",
      "|197568495640|25          |The Cozy Cafe        |24784                  |US     |Oskaloosa         |41.324|-92.646 |\n",
      "|163208757306|59          |Azalea Cafe          |96638                  |FR     |Paris             |48.871|2.294   |\n",
      "|42          |43          |The Food House       |47732                  |AT     |Vienna            |48.163|16.34   |\n",
      "|23          |24          |The Fisherman's Catch|47732                  |ES     |Barcelona         |41.396|2.163   |\n",
      "|51539607572 |21          |The Lazy Daisy       |6934                   |ES     |Barcelona         |41.387|2.174   |\n",
      "|257698037775|16          |The Spice Tree       |26468                  |US     |Morgantown        |39.631|-79.956 |\n",
      "|188978561036|13          |The Firehouse        |3642                   |US     |Atlantic Beach    |34.701|-76.747 |\n",
      "|77309411367 |40          |Crimson Cafe         |78190                  |AT     |Vienna            |48.213|16.357  |\n",
      "|128849018902|23          |The Hungry Pig       |5679                   |ES     |Barcelona         |41.389|2.171   |\n",
      "|223338299392|1           |Savoria              |36937                  |US     |Washington        |13.368|100.987 |\n",
      "|103079215115|12          |The Wooden Spoon     |4340                   |US     |Blythewood        |34.214|-80.974 |\n",
      "+------------+------------+---------------------+-----------------------+-------+------------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Round the lat and lng to three fractional values\n",
    "from pyspark.sql.functions import round\n",
    "updated_df = updated_df.withColumn(\"lat\",round(col(\"lat\"), 3))\n",
    "updated_df = updated_df.withColumn(\"lng\",round(col(\"lng\"), 3))\n",
    "updated_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "846f506d-3339-40ab-b855-5463cda8359d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+-----------------------+-------+------------------+------+--------+-------+\n",
      "|          id|franchise_id|      franchise_name|restaurant_franchise_id|country|              city|   lat|     lng|geohash|\n",
      "+------------+------------+--------------------+-----------------------+-------+------------------+------+--------+-------+\n",
      "|257698037796|          37|          Cafe Crepe|                  26468|     IT|             Milan|45.533|   9.171|   u0ne|\n",
      "| 25769803831|          56|    The Waffle House|                  72230|     FR|             Paris|48.873|   2.305|   u09w|\n",
      "| 85899345988|          69|      Dragonfly Cafe|                  18952|     NL|         Amsterdam|52.392|   4.911|   u176|\n",
      "|111669149758|          63|          Cafe Paris|                  84488|     NL|Amsterdam Zuidoost| 52.31|   4.942|   u179|\n",
      "|163208757268|          21|      The Lazy Daisy|                  96638|     US|          Columbus|40.115| -83.015|   dphu|\n",
      "|154618822662|           7|           Cafe Roma|                  41484|     US|             Tatum|33.382|-103.395|   9tym|\n",
      "|163208757290|          43|      The Food House|                  96638|     IT|             Milan|45.474|   9.224|   u0nd|\n",
      "|266287972361|          10|    The Golden Spoon|                  11263|     US|            Marina|36.684|-121.792|   9q92|\n",
      "|171798691894|          55|     The Steak House|                  65939|     GB|            London|51.502|     0.0|   gcpu|\n",
      "|197568495640|          25|       The Cozy Cafe|                  24784|     US|         Oskaloosa|41.324| -92.646|   9zq5|\n",
      "|163208757306|          59|         Azalea Cafe|                  96638|     FR|             Paris|48.871|   2.294|   u09w|\n",
      "|          42|          43|      The Food House|                  47732|     AT|            Vienna|48.163|   16.34|   u2e9|\n",
      "|          23|          24|The Fisherman's C...|                  47732|     ES|         Barcelona|41.396|   2.163|   sp3e|\n",
      "| 51539607572|          21|      The Lazy Daisy|                   6934|     ES|         Barcelona|41.387|   2.174|   sp3e|\n",
      "|257698037775|          16|      The Spice Tree|                  26468|     US|        Morgantown|39.631| -79.956|   dpp1|\n",
      "|188978561036|          13|       The Firehouse|                   3642|     US|    Atlantic Beach|34.701| -76.747|   dq1m|\n",
      "| 77309411367|          40|        Crimson Cafe|                  78190|     AT|            Vienna|48.213|  16.357|   u2ed|\n",
      "|128849018902|          23|      The Hungry Pig|                   5679|     ES|         Barcelona|41.389|   2.171|   sp3e|\n",
      "|223338299392|           1|             Savoria|                  36937|     US|        Washington|13.368| 100.987|   w4ru|\n",
      "|103079215115|          12|    The Wooden Spoon|                   4340|     US|        Blythewood|34.214| -80.974|   dnn6|\n",
      "+------------+------------+--------------------+-----------------------+-------+------------------+------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import geohash2\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a UDF to compute geohash, handling None values\n",
    "@udf(StringType())\n",
    "def safe_geohash(lat, lng):\n",
    "    if lat is not None and lng is not None:\n",
    "        return geohash2.encode(lat, lng, precision=4)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Register the UDF\n",
    "geohash_udf = safe_geohash\n",
    "\n",
    "# Apply the UDF to create a new column \"geohash\" in the dataframe\n",
    "geohash_df = updated_df.withColumn(\"geohash\", geohash_udf(col(\"lat\"), col(\"lng\")))\n",
    "\n",
    "\n",
    "geohash_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "04cdca19-8828-435e-8c71-c25e2e26636e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This line generates a list of file paths using rglob(\"*.parquet\"), \n",
    "which recursively searches for all files with the .parquet extension in the specified directory and its subdirectories. \n",
    "The list comprehension converts the Path objects to strings.\n",
    "After running this code, parquet_files will contain the paths to all .parquet files in the specified directory\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "parquet_files = []\n",
    "directory_path = f\"Weather_ds/weather/\"\n",
    "# Get a list of .parquet files in the specified directory\n",
    "parquet_files = [str(file) for file in Path(directory_path).rglob(\"*.parquet\")]\n",
    "\n",
    "len(parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "05a0127e-84d9-4b1d-a442-300fcea7d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I save all links to .parquet files in one list. Because when I read all the .parquet files at once, it takes a long time to load.\n",
    "# So I decided to read one file at a time. Then I delete duplicates. To implement such a method, you need to have a ready-made dataframe.\n",
    "# And each time read one file, process this file (remove duplicates), and fill new dataframes to the existing dataframe.\n",
    "weather_data = spark.read.parquet(parquet_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e3cb2c08-2309-47e8-9289-1a922cd06724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+----------+----------+-------+\n",
      "|     lng|    lat|avg_tmpr_f|avg_tmpr_c| wthr_date|geohash|\n",
      "+--------+-------+----------+----------+----------+-------+\n",
      "|-111.202|18.7496|      82.1|      27.8|2016-10-21|   9e2f|\n",
      "|-111.155| 18.755|      82.1|      27.8|2016-10-21|   9e2f|\n",
      "|-111.107|18.7604|      82.0|      27.8|2016-10-21|   9e2f|\n",
      "|-111.059|18.7657|      81.9|      27.7|2016-10-21|   9e34|\n",
      "|-111.012|18.7711|      81.8|      27.7|2016-10-21|   9e34|\n",
      "|-110.964|18.7764|      81.7|      27.6|2016-10-21|   9e34|\n",
      "|-110.916|18.7818|      81.6|      27.6|2016-10-21|   9e34|\n",
      "|-110.869|18.7871|      81.7|      27.6|2016-10-21|   9e34|\n",
      "|-110.821|18.7924|      81.8|      27.7|2016-10-21|   9e34|\n",
      "|-110.773|18.7977|      81.9|      27.7|2016-10-21|   9e34|\n",
      "|-110.726|18.8029|      81.9|      27.7|2016-10-21|   9e36|\n",
      "|-105.221|19.3026|      83.1|      28.4|2016-10-21|   9emj|\n",
      "|-105.173| 19.306|      82.9|      28.3|2016-10-21|   9emj|\n",
      "|-105.125|19.3094|      82.6|      28.1|2016-10-21|   9emj|\n",
      "|-105.077|19.3128|      82.1|      27.8|2016-10-21|   9emm|\n",
      "|-105.029|19.3162|      81.7|      27.6|2016-10-21|   9emm|\n",
      "| -104.98|19.3196|      80.9|      27.2|2016-10-21|   9emm|\n",
      "|-104.932|19.3229|      80.1|      26.7|2016-10-21|   9emm|\n",
      "|-104.884|19.3262|      79.4|      26.3|2016-10-21|   9emm|\n",
      "|-104.836|19.3295|      79.0|      26.1|2016-10-21|   9emm|\n",
      "+--------+-------+----------+----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To use weather and restaurants dataframes, you need a common value.\n",
    "In this case, our common value will be geohash.\n",
    "To do this, I create a new geohash column in the weather dataframe based on the lat and lng columns\n",
    "\"\"\"\n",
    "# Register the UDF\n",
    "weather_data_udf = safe_geohash\n",
    "\n",
    "# Apply the UDF to create a new column \"geohash\" in the dataframe\n",
    "weather_data = weather_data.withColumn(\"geohash\", weather_data_udf(col(\"lat\"), col(\"lng\")))\n",
    "\n",
    "weather_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "07c36707-99f6-4dd6-bb65-a73efae3db8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fc969b95-e197-4b32-805a-775cb9777b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cycle processes Parquet files. \n",
    "Each file is read into a DataFrame (current_data), duplicates are removed by the \"wthr_date\" and \"geohash\" columns, \n",
    "and then the data is merged into the common DataFrame weather_data using the union operation.\n",
    "After the loop completes, a duplicate removal operation is performed on the \"wthr_date\" and \"geohash\" columns.\n",
    "\n",
    "The overall result of the code is to combine data from several Parquet files into one DataFrame weather_data on the \"wthr_date\" and \"geohash\" columns\n",
    "\"\"\"\n",
    "weather_data = weather_data.dropDuplicates([\"wthr_date\",\"geohash\"])\n",
    "for i in range(1,len(parquet_files)):\n",
    "    current_data = spark.read.parquet(parquet_files[i])\n",
    "    current_data = current_data.withColumn(\"geohash\", weather_data_udf(col(\"lat\"), col(\"lng\")))\n",
    "    current_data = current_data.dropDuplicates([\"wthr_date\",\"geohash\"])\n",
    "    weather_data = weather_data.union(current_data)\n",
    "weather_data = weather_data.dropDuplicates([\"wthr_date\",\"geohash\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3718d3c6-bd17-4f8f-a2b6-07bfd3a7ec5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+----------+----------+-------+\n",
      "|     lng|    lat|avg_tmpr_f|avg_tmpr_c| wthr_date|geohash|\n",
      "+--------+-------+----------+----------+----------+-------+\n",
      "|-72.0244|18.4577|      74.6|      23.7|2016-10-21|   d7kc|\n",
      "|-105.308|20.3914|      74.2|      23.4|2016-10-21|   9eth|\n",
      "|-89.6325|20.9328|      79.6|      26.4|2016-10-21|   d58r|\n",
      "|-104.275| 20.918|      65.4|      18.6|2016-10-21|   9etz|\n",
      "| -77.941|20.4832|      81.1|      27.3|2016-10-21|   d78s|\n",
      "| -74.852|20.7737|      80.9|      27.2|2016-10-21|   d7dy|\n",
      "|-100.543|22.7064|      60.4|      15.8|2016-10-21|   9u09|\n",
      "|-79.8007|22.5267|      74.0|      23.3|2016-10-21|   dhp2|\n",
      "|-102.268|23.4083|      57.7|      14.3|2016-10-21|   9spm|\n",
      "|-109.762|23.0307|      81.3|      27.4|2016-10-21|   9s1g|\n",
      "|-99.2092|24.2582|      67.3|      19.6|2016-10-21|   9u36|\n",
      "|-102.269|24.2772|      61.1|      16.2|2016-10-21|   9sr6|\n",
      "|-98.4268|24.9637|      69.6|      20.9|2016-10-21|   9u6n|\n",
      "|-83.9856| 51.161|      34.9|       1.6|2016-10-21|   f1h7|\n",
      "|-67.4871|49.7812|      40.4|       4.7|2016-10-21|   f8b5|\n",
      "|-73.4231|50.6432|      37.6|       3.1|2016-10-21|   f35b|\n",
      "|-119.299|50.8037|      44.3|       6.8|2016-10-21|   c351|\n",
      "|-71.3662|51.0027|      40.2|       4.6|2016-10-21|   f3j6|\n",
      "|-79.3973| 52.058|      39.6|       4.2|2016-10-21|   f1r8|\n",
      "| -122.17|51.1604|      37.0|       2.8|2016-10-21|   c315|\n",
      "+--------+-------+----------+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After deleting duplicates.\n",
    "weather_data.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "accba20a-d0e3-4fe4-9b33-57b7c2786d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting dataframes weather_data and geohash_df using the left join. Common column is geohash\n",
    "joined_df = weather_data.join(geohash_df, on=\"geohash\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5fb7911b-b078-4745-85d4-f40830ed6074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+----------+----------+----------+----+------------+--------------+-----------------------+-------+----+----+----+\n",
      "|geohash|     lng|    lat|avg_tmpr_f|avg_tmpr_c| wthr_date|  id|franchise_id|franchise_name|restaurant_franchise_id|country|city| lat| lng|\n",
      "+-------+--------+-------+----------+----------+----------+----+------------+--------------+-----------------------+-------+----+----+----+\n",
      "|   d7kc|-72.0244|18.4577|      74.6|      23.7|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   9eth|-105.308|20.3914|      74.2|      23.4|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   d58r|-89.6325|20.9328|      79.6|      26.4|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   9etz|-104.275| 20.918|      65.4|      18.6|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   d78s| -77.941|20.4832|      81.1|      27.3|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   d7dy| -74.852|20.7737|      80.9|      27.2|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   9u09|-100.543|22.7064|      60.4|      15.8|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   dhp2|-79.8007|22.5267|      74.0|      23.3|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   9spm|-102.268|23.4083|      57.7|      14.3|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   9s1g|-109.762|23.0307|      81.3|      27.4|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   9u36|-99.2092|24.2582|      67.3|      19.6|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   9sr6|-102.269|24.2772|      61.1|      16.2|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   9u6n|-98.4268|24.9637|      69.6|      20.9|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   f1h7|-83.9856| 51.161|      34.9|       1.6|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   f8b5|-67.4871|49.7812|      40.4|       4.7|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   f35b|-73.4231|50.6432|      37.6|       3.1|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   c351|-119.299|50.8037|      44.3|       6.8|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   f3j6|-71.3662|51.0027|      40.2|       4.6|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   f1r8|-79.3973| 52.058|      39.6|       4.2|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "|   c315| -122.17|51.1604|      37.0|       2.8|2016-10-21|NULL|        NULL|          NULL|                   NULL|   NULL|NULL|NULL|NULL|\n",
      "+-------+--------+-------+----------+----------+----------+----+------------+--------------+-----------------------+-------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "51122b12-f321-44aa-b9b6-4cf926a2431e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------+----------+----------+----------+\n",
      "|country|              city|    franchise_name|geohash|avg_tmpr_f|avg_tmpr_c| wthr_date|\n",
      "+-------+------------------+------------------+-------+----------+----------+----------+\n",
      "|     US|          Marathon|           Savoria|   dhqk|      78.5|      25.8|2016-10-21|\n",
      "|     US|            Normal|    The Red Pepper|   dp0w|      45.6|       7.6|2016-10-21|\n",
      "|     US|       Bloomington|    The Red Pepper|   dp0w|      45.6|       7.6|2016-10-21|\n",
      "|     US|       Bloomington|The Gourmet Garden|   dp0w|      45.6|       7.6|2016-10-21|\n",
      "|     US|       Bloomington|    The Lazy Daisy|   dp0w|      45.6|       7.6|2016-10-21|\n",
      "|     US|         Woodstock|    The Hungry Pig|   dp90|      42.1|       5.6|2016-10-21|\n",
      "|     US|      Crystal Lake| The Blue Elephant|   dp90|      42.1|       5.6|2016-10-21|\n",
      "|     US|   Boiling Springs|  The Golden Spoon|   dnjx|      61.4|      16.3|2016-10-21|\n",
      "|     US|             Drums|   The Wine Cellar|   dr3c|      60.9|      16.1|2016-10-21|\n",
      "|     US|       Albuquerque|  The Wooden Spoon|   9whp|      54.9|      12.7|2016-10-21|\n",
      "|     US|       Albuquerque|   The Green Olive|   9whp|      54.9|      12.7|2016-10-21|\n",
      "|     US|       Albuquerque|   The Daily Grind|   9whp|      54.9|      12.7|2016-10-21|\n",
      "|     US|           Memphis|   The Green Olive|   dn20|      54.9|      12.7|2016-10-21|\n",
      "|     US|           Memphis|   The Flaming Wok|   dn20|      54.9|      12.7|2016-10-21|\n",
      "|     US|            Toledo| The Blue Elephant|   dpks|      47.9|       8.8|2016-10-21|\n",
      "|     US|            Maumee|    The Hungry Pig|   dpks|      47.9|       8.8|2016-10-21|\n",
      "|     US|Indian Rocks Beach|           Savoria|   dhvn|      76.1|      24.5|2016-10-21|\n",
      "|     US|        Mendenhall|    The Lazy Daisy|   dr44|      64.9|      18.3|2016-10-21|\n",
      "|     US|           Madison|   Basilica Bistro|   dp8m|      43.1|       6.2|2016-10-21|\n",
      "|     US|            Jasper|   The Daily Grind|   dn1b|      58.6|      14.8|2016-10-21|\n",
      "+-------+------------------+------------------+-------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Used inner to show same values\n",
    "joined_df = weather_data.join(geohash_df, on=\"geohash\", how=\"inner\")\n",
    "joined_df.select([\"country\", \"city\", \"franchise_name\", \"geohash\", \"avg_tmpr_f\", \"avg_tmpr_c\", \"wthr_date\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d14f1-884a-4b52-9dc0-7d6526b988f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
